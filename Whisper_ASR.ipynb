{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "mount_file_id": "1s1qao8xlw1If2ZaDTdcxIWxq2COqot8-",
      "authorship_tag": "ABX9TyOsKAqjnh2Vh9BTrduGWGsN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cxctis/Google-Colab-Experiment/blob/main/Whisper_ASR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# 1. Upload\n",
        "print(\"Please upload your file:\")\n",
        "uploaded = files.upload()\n",
        "# FIX: It should be .keys() with an 's'\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# 2. Convert to MP3\n",
        "# FIX: We use !ffmpeg (command line) because the 'ffmpeg' python library\n",
        "# often requires complex setup in Colab. This is much more reliable:\n",
        "audio_output = \"audio_converted.mp3\"\n",
        "!ffmpeg -i \"{file_name}\" -acodec libmp3lame -ar 44100 -ac 2 -ab 192k \"{audio_output}\" -y\n",
        "\n",
        "# 3. Load Whisper\n",
        "import whisper\n",
        "# Note: Ensure you ran !pip install openai-whisper in a previous cell\n",
        "model = whisper.load_model(\"medium\")\n",
        "print(f\"Transcribing {file_name}... this may take a few minutes.\")\n",
        "\n",
        "# 4. Transcribe\n",
        "# Added fp16=False to prevent warnings if you aren't using a GPU\n",
        "result = model.transcribe(audio_output, language=\"en\", fp16=False)\n",
        "\n",
        "print(\"\\n---DONE!---\\n\")\n",
        "print(result[\"text\"])\n",
        "\n",
        "# 5. Save\n",
        "with open(\"transcription.txt\", \"w\") as f:\n",
        "    f.write(result[\"text\"])\n",
        "\n",
        "print(\"\\nYour transcription has been saved as 'transcription.txt' in the files folder on the left.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KzUxuCPyOaYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "import whisper\n",
        "model = whisper.load_model(\"medium\")\n",
        "print(f\"Transcribing with timestamp...this might take a moment.\")\n",
        "\n",
        "result = model.transcribe(audio_output, language=\"en\", fp16=False)\n",
        "\n",
        "def format_timestamp(seconds: float):\n",
        "  td = datetime.timedelta(seconds=seconds)\n",
        "  total_seconds = int(td.total_seconds())\n",
        "  hours, reminder = divmod(total_seconds, 3600)\n",
        "  minutes, seconds_int = divmod(reminder, 60)\n",
        "  milliseconds = int((seconds -int(seconds)) * 1000)\n",
        "  return f\"{hours:02}:{minutes:02}:{seconds_int:02},{milliseconds:03}\"\n",
        "\n",
        "caption_text = \"\"\n",
        "for i, segment in enumerate(result['segments'], start=1):\n",
        "  start = format_timestamp(segment['start'])\n",
        "  end = format_timestamp(segment['end'])\n",
        "  text = segment['text'].strip()\n",
        "\n",
        "  caption_line = f\"{i}\\n{start} --> {end}\\n{text}\\n\\n\"\n",
        "  caption_text += caption_line\n",
        "\n",
        "print(\"\\n--- CAPTION PREVIEW ---\\n\")\n",
        "print(caption_text[:500] + \"...\") # Preview the first 500 chars\n",
        "\n",
        "with open(\"captions.srt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(caption_text)\n",
        "\n",
        "print(\"\\nSuccess! Your 'captions.srt' file is ready in the left folder.\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download('captions.srt')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1UuldJKvR20R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git yt-dlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TBgvPOroT-4J",
        "outputId": "4b4b9598-7787-4400-e1dd-5179ca1b6dcd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-_vpl8uhz\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-_vpl8uhz\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2026.2.21-py3-none-any.whl.metadata (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.10.0+cu128)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (4.67.3)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (3.6.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2025.3.0)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.13.1.3)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch->openai-whisper==20250625) (1.3.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper==20250625) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.3)\n",
            "Downloading yt_dlp-2026.2.21-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2026.2.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. INSTALL NECESSARY TOOLS\n",
        "!pip install git+https://github.com/openai/whisper.git yt-dlp\n",
        "!sudo apt update && sudo apt install ffmpeg\n",
        "\n",
        "import yt_dlp\n",
        "import whisper\n",
        "import datetime\n",
        "from google.colab import files\n",
        "\n",
        "# 2. INPUT THE URL\n",
        "video_url = input(\"Enter the YouTube Video URL: \")\n",
        "\n",
        "# 3. DOWNLOAD AUDIO ONLY\n",
        "print(\"Downloading audio from YouTube...\")\n",
        "ydl_opts = {\n",
        "    'format': 'm4a/bestaudio/best',\n",
        "    'outtmpl': 'youtube_audio.%(ext)s',\n",
        "    'postprocessors': [{\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'mp3',\n",
        "        'preferredquality': '192',\n",
        "    }],\n",
        "}\n",
        "\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.download([video_url])\n",
        "\n",
        "audio_file = \"youtube_audio.mp3\"\n",
        "\n",
        "# 4. TRANSCRIBE\n",
        "print(\"Loading model and transcribing...\")\n",
        "model = whisper.load_model(\"medium\")\n",
        "result = model.transcribe(audio_file, fp16=True) # Set to True for GPU speed\n",
        "\n",
        "# 5. FORMAT AS CAPTIONS (SRT)\n",
        "def format_timestamp(seconds: float):\n",
        "    td = datetime.timedelta(seconds=seconds)\n",
        "    total_seconds = int(td.total_seconds())\n",
        "    hours, remainder = divmod(total_seconds, 3600)\n",
        "    minutes, seconds_int = divmod(remainder, 60)\n",
        "    milliseconds = int((seconds - int(seconds)) * 1000)\n",
        "    return f\"{hours:02}:{minutes:02}:{seconds_int:02},{milliseconds:03}\"\n",
        "\n",
        "caption_text = \"\"\n",
        "for i, segment in enumerate(result['segments'], start=1):\n",
        "    start = format_timestamp(segment['start'])\n",
        "    end = format_timestamp(segment['end'])\n",
        "    text = segment['text'].strip()\n",
        "    caption_text += f\"{i}\\n{start} --> {end}\\n{text}\\n\\n\"\n",
        "\n",
        "# 6. SAVE AND DOWNLOAD\n",
        "file_output = \"youtube_transcript.srt\"\n",
        "with open(file_output, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(caption_text)\n",
        "\n",
        "print(f\"\\n--- DONE! ---\\nFile saved as {file_output}\")\n",
        "files.download(file_output)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DtJqJk9tXRiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FsGpPpgaXvx6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}