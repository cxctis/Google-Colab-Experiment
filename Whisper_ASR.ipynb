{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "mount_file_id": "1s1qao8xlw1If2ZaDTdcxIWxq2COqot8-",
      "authorship_tag": "ABX9TyOsKAqjnh2Vh9BTrduGWGsN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cxctis/Google-Colab-Experiment/blob/main/Whisper_ASR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# 1. Upload\n",
        "print(\"Please upload your file:\")\n",
        "uploaded = files.upload()\n",
        "# FIX: It should be .keys() with an 's'\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# 2. Convert to MP3\n",
        "# FIX: We use !ffmpeg (command line) because the 'ffmpeg' python library\n",
        "# often requires complex setup in Colab. This is much more reliable:\n",
        "audio_output = \"audio_converted.mp3\"\n",
        "!ffmpeg -i \"{file_name}\" -acodec libmp3lame -ar 44100 -ac 2 -ab 192k \"{audio_output}\" -y\n",
        "\n",
        "# 3. Load Whisper\n",
        "import whisper\n",
        "# Note: Ensure you ran !pip install openai-whisper in a previous cell\n",
        "model = whisper.load_model(\"medium\")\n",
        "print(f\"Transcribing {file_name}... this may take a few minutes.\")\n",
        "\n",
        "# 4. Transcribe\n",
        "# Added fp16=False to prevent warnings if you aren't using a GPU\n",
        "result = model.transcribe(audio_output, language=\"en\", fp16=False)\n",
        "\n",
        "print(\"\\n---DONE!---\\n\")\n",
        "print(result[\"text\"])\n",
        "\n",
        "# 5. Save\n",
        "with open(\"transcription.txt\", \"w\") as f:\n",
        "    f.write(result[\"text\"])\n",
        "\n",
        "print(\"\\nYour transcription has been saved as 'transcription.txt' in the files folder on the left.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 961
        },
        "collapsed": true,
        "id": "KzUxuCPyOaYF",
        "outputId": "770dd221-3e2d-4944-9b43-e23049137125"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-eace4fd8-285c-4ad2-9441-db0d456f557d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-eace4fd8-285c-4ad2-9441-db0d456f557d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving The-Knowledge-Centre-on-Interpretation_Media.mp4 to The-Knowledge-Centre-on-Interpretation_Media.mp4\n",
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'The-Knowledge-Centre-on-Interpretation_Media.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf62.3.100\n",
            "  Duration: 00:06:13.05, start: 0.000000, bitrate: 2177 kb/s\n",
            "  Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709), 1920x1080 [SAR 1:1 DAR 16:9], 2044 kb/s, 23.98 fps, 23.98 tbr, 24k tbn, 47.95 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : ISO Media file produced by Google Inc.\n",
            "      vendor_id       : [0][0][0][0]\n",
            "  Stream #0:1(spa): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 127 kb/s (default)\n",
            "    Metadata:\n",
            "      handler_name    : ISO Media file produced by Google Inc.\n",
            "      vendor_id       : [0][0][0][0]\n",
            "Stream mapping:\n",
            "  Stream #0:1 -> #0:0 (aac (native) -> mp3 (libmp3lame))\n",
            "Press [q] to stop, [?] for help\n",
            "Output #0, mp3, to 'audio_converted.mp3':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    TSSE            : Lavf58.76.100\n",
            "  Stream #0:0(spa): Audio: mp3, 44100 Hz, stereo, fltp, 192 kb/s (default)\n",
            "    Metadata:\n",
            "      handler_name    : ISO Media file produced by Google Inc.\n",
            "      vendor_id       : [0][0][0][0]\n",
            "      encoder         : Lavc58.134.100 libmp3lame\n",
            "size=    8745kB time=00:06:13.02 bitrate= 192.0kbits/s speed=51.4x    \n",
            "video:0kB audio:8744kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.008644%\n",
            "Transcribing The-Knowledge-Centre-on-Interpretation_Media.mp4... this may take a few minutes.\n",
            "\n",
            "---DONE!---\n",
            "\n",
            " Hello, my name is Cathy Pearson and I'm the project manager for Skic's new flagship project, the Knowledge Centre on Interpretation. You may have seen some of our promotional videos on Twitter or Facebook, but I would like to tell you a little bit more about this project. First of all, what is it? Well, it's a website, it's a platform which brings together or aims to bring together interpreters from around the world, but not just interpreters, also researchers, representatives of industry, trainers, students, all sorts of different people. Conference interpreters, yes, but also other types of interpreters, so interpreters who work in hospitals, in schools, in courts, for the police, for asylum seekers. We would like all of our interpreters, who are all family, to come into the fold and to be part of the Knowledge Centre on Interpretation. So it's a platform that has a lot of content in terms of conference interpreting and other types of interpreting. You can find, for example, on there all sorts of training resources. If you're a trainer or a student, you can find out what interpreting is all about, if you don't know anything about interpreting. But there's also much more detailed resources for people who are already interpreters, for example, terminology resources, or background reading, journals, and so on and so forth. So there's an awful lot of content, content which will constantly be updated as well by us, but also by our users. And that's what makes this platform very innovative, because it's also a collaborative platform. It's not just static content and information that we're giving, not at all. In fact, the main aim of the Knowledge Centre is to share information. So we share with you, but you share with us. You have a lot of know-how in your field of expertise, whatever it might be. And we would like to receive contributions. We want to learn from you as well. And so it should be a place where people can learn from each other and learn together about all the many facets of all the different types of interpreting. So why have we set this project up? Our core business, after all, is conference interpreting and conference interpreter training. But for all the other types of interpreting, and with recent refugee crisis, migration issues, those other types of interpreting are really becoming more and more prominent. There's more and more interpreting necessary as people move around more for all sorts of reasons, sometimes because they are asylum seekers fleeing from war. But there might be all sorts of other reasons. They might just have moved to another country and need help before they can, before they manage to learn the language of the country. So it was also a way of making a very small contribution, but a contribution nonetheless to the Commission's political priorities to try and help migrants and do our little bit. So it really is for everybody, everybody and anybody who has an interest in interpreting, from whichever perspective you might be taking it. So we would like it to be as open as possible. Now, as I said, it's a collaborative platform. So there's the more static part that has the content, but that can be added to by you, by the users. It would have to pass through the editorial team, which is me and the people working with me on the project. And then we can put the content that you generate on our site as user-generated content. But there's also collaborative spaces where you can set up a community to bring together people to talk about the one thing you're particularly interested in or something that's particularly happening in your country. At the moment, the site's in English, but it will move to a completely multilingual version during the course of next year. When I say completely multilingual, I am of course referring to the European Union official languages. I'm afraid we couldn't hope to cover all languages. That would be far too ambitious. But already now, you can set up a community in one of the European Union official languages to talk about something in particular. We live in a virtual globalized world, and this is a way of bringing people together, virtually, yes, but in communities to really talk about, discuss, and work on issues that are of interest to us and to the general world of interpreting. Those communities, you can set them up yourselves. Obviously, we as the editorial team moderate them to some extent, but they're much freer. And the forums as well. So there's really space for you to express freely what you want. There's also news and events. So any events that are happening in your region, in your area, or any news that you might have, you can send that in to us and we will publish it. So all you need to do is go to the Knowledge Center website. It's ec.europa.eu slash skic, S-C-I-C, hyphen K-C-I, for the Knowledge Center on Interpretation. So do join us. Sign up, and we'll look forward to seeing you there.\n",
            "\n",
            "Your transcription has been saved as 'transcription.txt' in the files folder on the left.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "import whisper\n",
        "model = whisper.load_model(\"medium\")\n",
        "print(f\"Transcribing with timestamp...this might take a moment.\")\n",
        "\n",
        "result = model.transcribe(audio_output, language=\"en\", fp16=False)\n",
        "\n",
        "def format_timestamp(seconds: float):\n",
        "  td = datetime.timedelta(seconds=seconds)\n",
        "  total_seconds = int(td.total_seconds())\n",
        "  hours, reminder = divmod(total_seconds, 3600)\n",
        "  minutes, seconds_int = divmod(reminder, 60)\n",
        "  milliseconds = int((seconds -int(seconds)) * 1000)\n",
        "  return f\"{hours:02}:{minutes:02}:{seconds_int:02},{milliseconds:03}\"\n",
        "\n",
        "caption_text = \"\"\n",
        "for i, segment in enumerate(result['segments'], start=1):\n",
        "  start = format_timestamp(segment['start'])\n",
        "  end = format_timestamp(segment['end'])\n",
        "  text = segment['text'].strip()\n",
        "\n",
        "  caption_line = f\"{i}\\n{start} --> {end}\\n{text}\\n\\n\"\n",
        "  caption_text += caption_line\n",
        "\n",
        "print(\"\\n--- CAPTION PREVIEW ---\\n\")\n",
        "print(caption_text[:500] + \"...\") # Preview the first 500 chars\n",
        "\n",
        "with open(\"captions.srt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(caption_text)\n",
        "\n",
        "print(\"\\nSuccess! Your 'captions.srt' file is ready in the left folder.\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download('captions.srt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "collapsed": true,
        "id": "1UuldJKvR20R",
        "outputId": "0fba255c-af02-4140-f348-d811c27f5653"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribing with timestamp...this might take a moment.\n",
            "\n",
            "--- CAPTION PREVIEW ---\n",
            "\n",
            "1\n",
            "00:00:00,000 --> 00:00:08,080\n",
            "Hello, my name is Cathy Pearson and I'm the project manager for Skic's new flagship project,\n",
            "\n",
            "2\n",
            "00:00:08,080 --> 00:00:11,800\n",
            "the Knowledge Centre on Interpretation.\n",
            "\n",
            "3\n",
            "00:00:11,800 --> 00:00:19,000\n",
            "You may have seen some of our promotional videos on Twitter or Facebook, but I would\n",
            "\n",
            "4\n",
            "00:00:19,000 --> 00:00:22,719\n",
            "like to tell you a little bit more about this project.\n",
            "\n",
            "5\n",
            "00:00:22,719 --> 00:00:25,160\n",
            "First of all, what is it?\n",
            "\n",
            "6\n",
            "00:00:25,320 --> 00:00:32,320\n",
            "Well...\n",
            "\n",
            "Success! Your 'captions.srt' file is ready in the left folder.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6818f992-f4f0-4273-89f7-3ddfe539030f\", \"captions.srt\", 7524)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git yt-dlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TBgvPOroT-4J",
        "outputId": "4b4b9598-7787-4400-e1dd-5179ca1b6dcd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-_vpl8uhz\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-_vpl8uhz\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2026.2.21-py3-none-any.whl.metadata (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.10.0+cu128)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (4.67.3)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (3.6.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2025.3.0)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.13.1.3)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch->openai-whisper==20250625) (1.3.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper==20250625) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.3)\n",
            "Downloading yt_dlp-2026.2.21-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2026.2.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. INSTALL NECESSARY TOOLS\n",
        "!pip install git+https://github.com/openai/whisper.git yt-dlp\n",
        "!sudo apt update && sudo apt install ffmpeg\n",
        "\n",
        "import yt_dlp\n",
        "import whisper\n",
        "import datetime\n",
        "from google.colab import files\n",
        "\n",
        "# 2. INPUT THE URL\n",
        "video_url = input(\"Enter the YouTube Video URL: \")\n",
        "\n",
        "# 3. DOWNLOAD AUDIO ONLY\n",
        "print(\"Downloading audio from YouTube...\")\n",
        "ydl_opts = {\n",
        "    'format': 'm4a/bestaudio/best',\n",
        "    'outtmpl': 'youtube_audio.%(ext)s',\n",
        "    'postprocessors': [{\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'mp3',\n",
        "        'preferredquality': '192',\n",
        "    }],\n",
        "}\n",
        "\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.download([video_url])\n",
        "\n",
        "audio_file = \"youtube_audio.mp3\"\n",
        "\n",
        "# 4. TRANSCRIBE\n",
        "print(\"Loading model and transcribing...\")\n",
        "model = whisper.load_model(\"medium\")\n",
        "result = model.transcribe(audio_file, fp16=True) # Set to True for GPU speed\n",
        "\n",
        "# 5. FORMAT AS CAPTIONS (SRT)\n",
        "def format_timestamp(seconds: float):\n",
        "    td = datetime.timedelta(seconds=seconds)\n",
        "    total_seconds = int(td.total_seconds())\n",
        "    hours, remainder = divmod(total_seconds, 3600)\n",
        "    minutes, seconds_int = divmod(remainder, 60)\n",
        "    milliseconds = int((seconds - int(seconds)) * 1000)\n",
        "    return f\"{hours:02}:{minutes:02}:{seconds_int:02},{milliseconds:03}\"\n",
        "\n",
        "caption_text = \"\"\n",
        "for i, segment in enumerate(result['segments'], start=1):\n",
        "    start = format_timestamp(segment['start'])\n",
        "    end = format_timestamp(segment['end'])\n",
        "    text = segment['text'].strip()\n",
        "    caption_text += f\"{i}\\n{start} --> {end}\\n{text}\\n\\n\"\n",
        "\n",
        "# 6. SAVE AND DOWNLOAD\n",
        "file_output = \"youtube_transcript.srt\"\n",
        "with open(file_output, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(caption_text)\n",
        "\n",
        "print(f\"\\n--- DONE! ---\\nFile saved as {file_output}\")\n",
        "files.download(file_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "DtJqJk9tXRiu",
        "outputId": "3d4d6fe3-6633-48cb-a7f8-74e65a5a2adf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-urxhjvgw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-urxhjvgw\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.12/dist-packages (2026.2.21)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.10.0+cu128)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (4.67.3)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (3.6.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2025.3.0)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.13.1.3)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch->openai-whisper==20250625) (1.3.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper==20250625) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.3)\n",
            "Hit:1 https://cli.github.com/packages stable InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "66 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 66 not upgraded.\n",
            "Enter the YouTube Video URL: https://youtu.be/F9_diBeynPM?si=M8Xmqtk-AD5Q512k\n",
            "Downloading audio from YouTube...\n",
            "[youtube] Extracting URL: https://youtu.be/F9_diBeynPM?si=M8Xmqtk-AD5Q512k\n",
            "[youtube] F9_diBeynPM: Downloading webpage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] No supported JavaScript runtime could be found. Only deno is enabled by default; to use another runtime add  --js-runtimes RUNTIME[:PATH]  to your command/config. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] F9_diBeynPM: Downloading android vr player API JSON\n",
            "[info] F9_diBeynPM: Downloading 1 format(s): 140\n",
            "[download] Destination: youtube_audio.m4a\n",
            "[download] 100% of    1.32MiB in 00:00:00 at 2.15MiB/s   \n",
            "[FixupM4a] Correcting container of \"youtube_audio.m4a\"\n",
            "[ExtractAudio] Destination: youtube_audio.mp3\n",
            "Deleting original file youtube_audio.m4a (pass -k to keep)\n",
            "Loading model and transcribing...\n",
            "\n",
            "--- DONE! ---\n",
            "File saved as youtube_transcript.srt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_26ffd2e6-1715-4eeb-9b7e-7b372b3cfa2d\", \"youtube_transcript.srt\", 1381)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FsGpPpgaXvx6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}